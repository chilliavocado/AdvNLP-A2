{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experiments to reproduce results in the paper.\n",
    "\n",
    "The approached used is based on the information from https://huggingface.co/docs/transformers/en/model_doc/dpr.\n",
    "The Natural Questions dataset from https://huggingface.co/datasets/vocab-transformers/wiki-en-passages-20210101 (only 10k)\n",
    "\n",
    "The question answer datasets\n",
    "* https://huggingface.co/datasets/google-research-datasets/nq_open\n",
    "* https://huggingface.co/datasets/mandarjoshi/trivia_qa\n",
    "* https://huggingface.co/datasets/Stanford/web_questions\n",
    "* https://huggingface.co/datasets/CogComp/trec\n",
    "* https://huggingface.co/datasets/rajpurkar/squad\n",
    "\n",
    "HuggingFace pretrained encoders enpoints\n",
    "* Single\n",
    "- facebook/dpr-question_encoder-single-nq-base\n",
    "* * facebook/dpr-ctx_encoder-single-nq-base\n",
    "* * facebook/dpr-ctx_encoder-single-nq-base\n",
    "* Multi\n",
    "* * facebook/dpr-question_encoder-multiset-base\n",
    "* * facebook/dpr-ctx_encoder-multiset-base\n",
    "* * facebook/dpr-ctx_encoder-multiset-base\n",
    "\n",
    "The experiment code is a combindation of all research including HuggingFace which encoders to use, Pyserini (used for evaluation) and ChatGPT for general python questions.\n",
    "\n",
    "The evaluation method is taken from Pysrini, which is taken from the DPR repository.\n",
    "\n",
    "Only Natural Question single is experimented using the wikipedia dataset. Other question and answer datasets seem to use different passages and too much to donwload. This helps to understand how the process is done for reproducing DPR results.\n",
    "\n",
    "<b>Other attempt using Pysrini</b>\n",
    "\n",
    "The index data is available in the Pysrini codebase described at https://github.com/castorini/pyserini/blob/master/docs/experiments-dpr.md. However, this dataset is 76GB, which is too large to store and process. The first command failed at file extraction. Many other attempts to use other index/dataset or increase compute resources to no avail."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sagemaker-user/AdvNLP/A2-DPR/venv/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "# import libraries\n",
    "import os\n",
    "import numpy as np\n",
    "import torch\n",
    "from datasets import load_dataset\n",
    "from transformers import DPRQuestionEncoder, DPRContextEncoder, DPRContextEncoderTokenizer\n",
    "from rank_bm25 import BM25Okapi\n",
    "import regex\n",
    "import unicodedata\n",
    "from datasets import load_from_disk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" \n",
    "Modified from https://github.com/castorini/pyserini/blob/master/pyserini/eval/evaluate_dpr_retrieval.py\n",
    "\"\"\"\n",
    "\n",
    "class Tokens(object):\n",
    "    \"\"\"A class to represent a list of tokenized text.\"\"\"\n",
    "    TEXT = 0\n",
    "    TEXT_WS = 1\n",
    "    SPAN = 2\n",
    "    POS = 3\n",
    "    LEMMA = 4\n",
    "    NER = 5\n",
    "\n",
    "    def __init__(self, data, annotators, opts=None):\n",
    "        self.data = data\n",
    "        self.annotators = annotators\n",
    "        self.opts = opts or {}\n",
    "\n",
    "    def __len__(self):\n",
    "        \"\"\"The number of tokens.\"\"\"\n",
    "        return len(self.data)\n",
    "\n",
    "    def slice(self, i=None, j=None):\n",
    "        \"\"\"Return a view of the list of tokens from [i, j).\"\"\"\n",
    "        new_tokens = copy.copy(self)\n",
    "        new_tokens.data = self.data[i: j]\n",
    "        return new_tokens\n",
    "\n",
    "    def untokenize(self):\n",
    "        \"\"\"Returns the original text (with whitespace reinserted).\"\"\"\n",
    "        return ''.join([t[self.TEXT_WS] for t in self.data]).strip()\n",
    "\n",
    "    def words(self, uncased=False):\n",
    "        \"\"\"Returns a list of the text of each token\n",
    "        Args:\n",
    "            uncased: lower cases text\n",
    "        \"\"\"\n",
    "        if uncased:\n",
    "            return [t[self.TEXT].lower() for t in self.data]\n",
    "        else:\n",
    "            return [t[self.TEXT] for t in self.data]\n",
    "\n",
    "    def offsets(self):\n",
    "        \"\"\"Returns a list of [start, end) character offsets of each token.\"\"\"\n",
    "        return [t[self.SPAN] for t in self.data]\n",
    "\n",
    "    def pos(self):\n",
    "        \"\"\"Returns a list of part-of-speech tags of each token.\n",
    "        Returns None if this annotation was not included.\n",
    "        \"\"\"\n",
    "        if 'pos' not in self.annotators:\n",
    "            return None\n",
    "        return [t[self.POS] for t in self.data]\n",
    "\n",
    "    def lemmas(self):\n",
    "        \"\"\"Returns a list of the lemmatized text of each token.\n",
    "        Returns None if this annotation was not included.\n",
    "        \"\"\"\n",
    "        if 'lemma' not in self.annotators:\n",
    "            return None\n",
    "        return [t[self.LEMMA] for t in self.data]\n",
    "\n",
    "    def entities(self):\n",
    "        \"\"\"Returns a list of named-entity-recognition tags of each token.\n",
    "        Returns None if this annotation was not included.\n",
    "        \"\"\"\n",
    "        if 'ner' not in self.annotators:\n",
    "            return None\n",
    "        return [t[self.NER] for t in self.data]\n",
    "\n",
    "    def ngrams(self, n=1, uncased=False, filter_fn=None, as_strings=True):\n",
    "        \"\"\"Returns a list of all ngrams from length 1 to n.\n",
    "        Args:\n",
    "            n: upper limit of ngram length\n",
    "            uncased: lower cases text\n",
    "            filter_fn: user function that takes in an ngram list and returns\n",
    "              True or False to keep or not keep the ngram\n",
    "            as_string: return the ngram as a string vs list\n",
    "        \"\"\"\n",
    "\n",
    "        def _skip(gram):\n",
    "            if not filter_fn:\n",
    "                return False\n",
    "            return filter_fn(gram)\n",
    "\n",
    "        words = self.words(uncased)\n",
    "        ngrams = [(s, e + 1)\n",
    "                  for s in range(len(words))\n",
    "                  for e in range(s, min(s + n, len(words)))\n",
    "                  if not _skip(words[s:e + 1])]\n",
    "\n",
    "        # Concatenate into strings\n",
    "        if as_strings:\n",
    "            ngrams = ['{}'.format(' '.join(words[s:e])) for (s, e) in ngrams]\n",
    "\n",
    "        return ngrams\n",
    "\n",
    "    def entity_groups(self):\n",
    "        \"\"\"Group consecutive entity tokens with the same NER tag.\"\"\"\n",
    "        entities = self.entities()\n",
    "        if not entities:\n",
    "            return None\n",
    "        non_ent = self.opts.get('non_ent', 'O')\n",
    "        groups = []\n",
    "        idx = 0\n",
    "        while idx < len(entities):\n",
    "            ner_tag = entities[idx]\n",
    "            # Check for entity tag\n",
    "            if ner_tag != non_ent:\n",
    "                # Chomp the sequence\n",
    "                start = idx\n",
    "                while (idx < len(entities) and entities[idx] == ner_tag):\n",
    "                    idx += 1\n",
    "                groups.append((self.slice(start, idx).untokenize(), ner_tag))\n",
    "            else:\n",
    "                idx += 1\n",
    "        return groups\n",
    "\n",
    "# Tokenizer base class\n",
    "class Tokenizer(object):\n",
    "    \"\"\"Base tokenizer class.\n",
    "    Tokenizers implement tokenize, which should return a Tokens class.\n",
    "    \"\"\"\n",
    "\n",
    "    def tokenize(self, text):\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def shutdown(self):\n",
    "        pass\n",
    "\n",
    "    def __del__(self):\n",
    "        self.shutdown()\n",
    "\n",
    "# Tokeniser for question and passage tokenization for checking matching answers\n",
    "class SimpleTokenizer(Tokenizer):\n",
    "    ALPHA_NUM = r'[\\p{L}\\p{N}\\p{M}]+'\n",
    "    NON_WS = r'[^\\p{Z}\\p{C}]'\n",
    "\n",
    "    def __init__(self, **kwargs):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            annotators: None or empty set (only tokenizes).\n",
    "        \"\"\"\n",
    "        self._regexp = regex.compile(\n",
    "            '(%s)|(%s)' % (self.ALPHA_NUM, self.NON_WS),\n",
    "            flags=regex.IGNORECASE + regex.UNICODE + regex.MULTILINE\n",
    "        )\n",
    "        if len(kwargs.get('annotators', {})) > 0:\n",
    "            logger.warning('%s only tokenizes! Skipping annotators: %s' %\n",
    "                           (type(self).__name__, kwargs.get('annotators')))\n",
    "        self.annotators = set()\n",
    "\n",
    "    def tokenize(self, text):\n",
    "        data = []\n",
    "        matches = [m for m in self._regexp.finditer(text)]\n",
    "        for i in range(len(matches)):\n",
    "            # Get text\n",
    "            token = matches[i].group()\n",
    "\n",
    "            # Get whitespace\n",
    "            span = matches[i].span()\n",
    "            start_ws = span[0]\n",
    "            if i + 1 < len(matches):\n",
    "                end_ws = matches[i + 1].span()[0]\n",
    "            else:\n",
    "                end_ws = span[1]\n",
    "\n",
    "            # Format data\n",
    "            data.append((\n",
    "                token,\n",
    "                text[start_ws: end_ws],\n",
    "                span,\n",
    "            ))\n",
    "        return Tokens(data, self.annotators)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Load DPR Models - 'single' or 'multi' from HuggingFace\n",
    "def load_dpr_models(mode='single'):\n",
    "    if mode == 'single':\n",
    "        question_encoder = DPRQuestionEncoder.from_pretrained(\"facebook/dpr-question_encoder-single-nq-base\")\n",
    "        context_encoder = DPRContextEncoder.from_pretrained(\"facebook/dpr-ctx_encoder-single-nq-base\")\n",
    "        tokenizer = DPRContextEncoderTokenizer.from_pretrained(\"facebook/dpr-ctx_encoder-single-nq-base\")\n",
    "    elif mode == 'multi':\n",
    "        question_encoder = DPRQuestionEncoder.from_pretrained(\"facebook/dpr-question_encoder-multiset-base\")\n",
    "        context_encoder = DPRContextEncoder.from_pretrained(\"facebook/dpr-ctx_encoder-multiset-base\")\n",
    "        tokenizer = DPRContextEncoderTokenizer.from_pretrained(\"facebook/dpr-ctx_encoder-multiset-base\")\n",
    "    else:\n",
    "        raise ExceptionType(\"Invalid mode: Try 'single' or multi'\")\n",
    "\n",
    "    return tokenizer, question_encoder, context_encoder\n",
    "\n",
    "# Encode the corpus using the encoders\n",
    "def encode_corpus(corpus, context_encoder, tokenizer):\n",
    "    context_embeddings = []\n",
    "    for context in corpus:\n",
    "        inputs = tokenizer(context, return_tensors=\"pt\", padding=True, truncation=True)\n",
    "        with torch.no_grad():\n",
    "            context_embedding = context_encoder(**inputs).pooler_output\n",
    "        context_embeddings.append(context_embedding)\n",
    "    return torch.vstack(context_embeddings)\n",
    "\n",
    "# Retrieve passage indices for top K using the context (or potential answers) and question embeddings\n",
    "def retrieve_passages(query, corpus, context_embeddings, question_encoder, tokenizer, top_k=100):\n",
    "    inputs = tokenizer([query], return_tensors=\"pt\", padding=True, truncation=True)\n",
    "    with torch.no_grad():\n",
    "        question_embedding = question_encoder(**inputs).pooler_output\n",
    "\n",
    "    # Compute similarities\n",
    "    similarities = torch.matmul(question_embedding, context_embeddings.T).squeeze(0).cpu().numpy()\n",
    "    dpr_indices = np.argsort(similarities)[::-1][:top_k]\n",
    "\n",
    "    return dpr_indices\n",
    "\n",
    "# Modified based on https://github.com/castorini/pyserini/blob/master/pyserini/eval/evaluate_dpr_retrieval.py\n",
    "def has_answers(passages, answers):\n",
    "    tokenizer = SimpleTokenizer()\n",
    "\n",
    "    for p in passages:\n",
    "        passage = \" \".join(p)\n",
    "        text = unicodedata.normalize('NFD', passage)\n",
    "        text = tokenizer.tokenize(text).words(uncased=True)\n",
    "\n",
    "        for ans in answers:\n",
    "            ans = unicodedata.normalize('NFD', ans)\n",
    "            ans = tokenizer.tokenize(ans).words(uncased=True)\n",
    "            for i in range(0, len(text) - len(ans) + 1):\n",
    "                if ans == text[i: i + len(ans)]:\n",
    "                    return True\n",
    "                    \n",
    "    return False\n",
    "\n",
    "# Calculate accuracy\n",
    "def calculate_accuracy(retrieved_indices, answers):\n",
    "    correct = sum(1 for idx in retrieved_indices if answers[idx] in answers)\n",
    "    return correct / len(answers) * 100\n",
    "\n",
    "# calcuate the scores for the given dataset. 1 for having a correct answer and 0 for none for each question\n",
    "def scoreTopKs(dataset_name, dataset, answers, corpus, context_embeddings, question_encoder, tokenizer):\n",
    "    # initialise accuracy score list for questions\n",
    "    q_dpr_20 = []\n",
    "    q_dpr_100 = []\n",
    "    \n",
    "    for idx, question in enumerate(dataset['question']): \n",
    "\n",
    "        # Retrieve passages using DPR\n",
    "        dpr_20_indices = retrieve_passages(question, corpus, context_embeddings, question_encoder, tokenizer, top_k=20)\n",
    "        dpr_100_indices = retrieve_passages(question, corpus, context_embeddings, question_encoder, tokenizer, top_k=100)\n",
    "\n",
    "        answer = answers[idx]\n",
    "\n",
    "        dpr_20_passges = [corpus for i in dpr_20_indices]\n",
    "        dpr_100_passges = [corpus for i in dpr_100_indices]\n",
    "\n",
    "        q_dpr_20.append(1 if has_answers(dpr_20_passges, answer) else 0)\n",
    "        q_dpr_100.append(1 if has_answers(dpr_100_passges, answer) else 0)\n",
    "        \n",
    "    # final overall accuracy - average of accuracy for each question\n",
    "    f_dpr_20 = sum(q_dpr_20) / len(q_dpr_20)\n",
    "    f_dpr_100 = sum(q_dpr_100) / len(q_dpr_100)\n",
    "\n",
    "    print('q_dpr_20 len:', len(q_dpr_20))\n",
    "    print(q_dpr_20)\n",
    "    print('\\n')\n",
    "\n",
    "    print('q_dpr_100 len:', len(q_dpr_100))\n",
    "    print(q_dpr_100)\n",
    "    print('\\n')\n",
    "\n",
    "    return f_dpr_20, f_dpr_100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define datasets\n",
    "datasets = {\n",
    "    \"Natural Questions\": \"nq_open\",\n",
    "    \"TriviaQA\": \"trivia_qa\",\n",
    "    \"WebQuestions\": \"web_questions\",\n",
    "    \"SQuAD\": \"squad\",\n",
    "    \"TREC\": \"trec\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# accuracy scores for each dataaset with single and multi\n",
    "acc_dpr_20_single = {}\n",
    "acc_dpr_100_single = {}\n",
    "acc_dpr_20_multi = {}\n",
    "acc_dpr_100_multi = {}\n",
    "\n",
    "# Initialise metrics for each dataset\n",
    "for name in datasets.items():\n",
    "    acc_dpr_20_single[name] = []\n",
    "    acc_dpr_100_single[name] = []\n",
    "    acc_dpr_20_multi[name] = []\n",
    "    acc_dpr_100_multi[name] = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at facebook/dpr-question_encoder-single-nq-base were not used when initializing DPRQuestionEncoder: ['question_encoder.bert_model.pooler.dense.bias', 'question_encoder.bert_model.pooler.dense.weight']\n",
      "- This IS expected if you are initializing DPRQuestionEncoder from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DPRQuestionEncoder from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of the model checkpoint at facebook/dpr-ctx_encoder-single-nq-base were not used when initializing DPRContextEncoder: ['ctx_encoder.bert_model.pooler.dense.bias', 'ctx_encoder.bert_model.pooler.dense.weight']\n",
      "- This IS expected if you are initializing DPRContextEncoder from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DPRContextEncoder from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n",
      "The tokenizer class you load from this checkpoint is 'DPRQuestionEncoderTokenizer'. \n",
      "The class this function is called from is 'DPRContextEncoderTokenizer'.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "tokenizer, question_encoder, context_encoder = load_dpr_models('single')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at facebook/dpr-question_encoder-multiset-base were not used when initializing DPRQuestionEncoder: ['question_encoder.bert_model.pooler.dense.bias', 'question_encoder.bert_model.pooler.dense.weight']\n",
      "- This IS expected if you are initializing DPRQuestionEncoder from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DPRQuestionEncoder from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of the model checkpoint at facebook/dpr-ctx_encoder-multiset-base were not used when initializing DPRContextEncoder: ['ctx_encoder.bert_model.pooler.dense.bias', 'ctx_encoder.bert_model.pooler.dense.weight']\n",
      "- This IS expected if you are initializing DPRContextEncoder from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DPRContextEncoder from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n",
      "The tokenizer class you load from this checkpoint is 'DPRQuestionEncoderTokenizer'. \n",
      "The class this function is called from is 'DPRContextEncoderTokenizer'.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "multi_tokenizer, multi_question_encoder, multi_context_encoder = load_dpr_models('multi')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the wiki corpus\n",
    "passage_dataset = load_from_disk(os.path.join(\"passages\", 'wiki-20210101-10000p'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'id': ['10577', '10577', '10577', '10577', '10577'],\n",
       " 'title': ['Finland', 'Finland', 'Finland', 'Finland', 'Finland'],\n",
       " 'views': [3513, 3513, 3513, 3513, 3513],\n",
       " 'langs': [305, 305, 305, 305, 305],\n",
       " 'text': ['Finland ( ; , ), officially the Republic of Finland (, ), is a Nordic country located in Northern Europe. Finland shares land borders with Sweden to the west, Russia to the east, and Norway to the north and is defined by the Gulf of Bothnia to the west and the Gulf of Finland to the south that are part of the Baltic Sea.',\n",
       "  'Finland has a population of approximately 5.5 million, making it the 25th-most populous country in Europe. The main language is Finnish, a Finnic language of the Uralic language family. Swedish is the second official language of Finland, and is mainly spoken in certain coastal areas of the country and on Åland. Finland is a parliamentary republic consisting of 19 regions and 310 municipalities. The climate varies significantly relative to latitude, from Southern Finland\\'s humid continental climate to the boreal climate of the north. Finland is primarily a boreal forest biome, with more than 180,000 recorded lakes, leading to the denomination \"the land of a thousand lakes\". With an area of , Finland is the eighth-largest country in Europe, and the most sparsely populated country in the European Union. Helsinki, the capital of Finland, is the largest metropolitan area with over 1.5 million people, which produces a third of the country\\'s GDP. Tampere and Turku are the next largest urban areas.',\n",
       "  'Finland was inhabited around 9000\\xa0BC after the Last Glacial Period. The Stone Age introduced several different ceramic styles and cultures. The Bronze Age and Iron Age were characterised by extensive contacts with other cultures in Fennoscandia and the Baltic region. From the late 13th century, Finland gradually became an integral part of Sweden as a consequence of the Northern Crusades, the legacy of which is reflected in the prevalence of the Swedish language and its official status. In 1809, as a result of the Finnish War, Finland was annexed by the Russian Empire as the autonomous Grand Duchy of Finland, during which Finnish art flourished and the idea of independence began to take hold. In 1906, Finland became the first European state to grant all adult citizens the right to vote, and the first in the world to give all adult citizens the right to run for public office. Nicholas II, the last Tsar of Russian Empire, tried to russify Finland and also terminate its political autonomy, but after the 1917 Russian Revolution, Finland declared itself independent from the empire. In 1918, the fledgling state was divided by the Finnish Civil War. During World War II, Finland fought the Soviet Union in the Winter War and the Continuation War, and Nazi Germany in the Lapland War. After the wars, Finland lost part of its territory, but maintained its independence.',\n",
       "  'Finland largely remained an agrarian country until the 1950s. After World War II, the country rapidly industrialised and developed an advanced economy, while building an extensive welfare state based on the Nordic model, resulting in widespread prosperity and a high per capita income. Finland joined the United Nations in 1955 and adopted an official policy of neutrality. Finland joined the OECD in 1969, the NATO Partnership for Peace in 1994, the European Union in 1995, the Euro-Atlantic Partnership Council in 1997, and the Eurozone at its inception in 1999.',\n",
       "  'Finland is a top performer in numerous metrics of national performance, including education, economic competitiveness, civil liberties, quality of life and human development. In 2015, Finland was ranked first in the World Human Capital and the Press Freedom Index and as the most stable country in the world during 2011–2016 in the Fragile States Index, and second in the Global Gender Gap Report. It also ranked first on the World Happiness Report report for 2018, 2019 and 2020.']}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "passage_dataset[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = passage_dataset['text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "context_embeddings = encode_corpus(corpus, context_encoder, tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "multi_context_embeddings = encode_corpus(corpus, multi_context_encoder, multi_tokenizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experiements: Passage Retrieval"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Natural Questions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load data for Natural Questions\n",
    "# data schema: https://huggingface.co/datasets/google-research-datasets/nq_open\n",
    "name = \"Natural Questions\"\n",
    "nq_dataset = load_from_disk(os.path.join(\"datasets\", 'nq_open'))\n",
    "nq_answers = nq_dataset['answer']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "nq_dpr_20, nq_dpr_100 = scoreTopKs(name, nq_dataset, nq_answers, corpus, context_embeddings, question_encoder, tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Single results\")\n",
    "print(nq_dpr_20, nq_dpr_100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "nq_multi_dpr_20, nq_multi_dpr_100 = scoreTopKs(name, answers, corpus, multi_context_embeddings, multi_question_encoder, multi_tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Multi results\")\n",
    "print(nq_dpr_20, nq_dpr_100)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
