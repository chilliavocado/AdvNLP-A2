{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Downloading dataset for DPR from HuggingFace\n",
    "\n",
    "This notebook is to download the question and answering dataset to reproduce the results in the Dense Passage Retrieval for Open-Domain Question Answering paper.\n",
    "\n",
    "These datasets are sourced from HuggingFace and maybe different from the orginal dataset.\n",
    "\n",
    "The datasets and the pretrained DPR models are hosted on HuggingFace.\n",
    "\n",
    "The dataset will be stored in the datasets/ directory.\n",
    "\n",
    "The Wikipedia passage is downloaded from https://huggingface.co/datasets/vocab-transformers/wiki-en-passages-20210101.\n",
    "Only downloaded 10k passages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving the dataset (1/1 shards): 100%|██████████| 10000/10000 [00:00<00:00, 850323.16 examples/s]\n",
      "Saving the dataset (1/1 shards): 100%|██████████| 10000/10000 [00:00<00:00, 329032.12 examples/s]\n",
      "Saving the dataset (1/1 shards): 100%|██████████| 3778/3778 [00:00<00:00, 533574.00 examples/s]\n",
      "Saving the dataset (1/1 shards): 100%|██████████| 10000/10000 [00:00<00:00, 148020.33 examples/s]\n",
      "Saving the dataset (1/1 shards): 100%|██████████| 5452/5452 [00:00<00:00, 72672.96 examples/s]\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from datasets import load_dataset\n",
    "\n",
    "# Download databaset from HuggingFace\n",
    "# Source of info: https://huggingface.co/facebook/dpr-ctx_encoder-multiset-base\n",
    "datasets = {\n",
    "    \"Natural Questions\": \"nq_open\",\n",
    "    \"TriviaQA\": \"trivia_qa\",\n",
    "    \"WebQuestions\": \"web_questions\",\n",
    "    \"SQuAD\": \"squad\",\n",
    "    \"TREC\": \"trec\"\n",
    "}\n",
    "\n",
    "os.makedirs(\"datasets\", exist_ok=True)\n",
    "\n",
    "# Download and save each dataset\n",
    "# 10000 as per results shown in the paper for good performance\n",
    "for name, dataset_key in datasets.items():\n",
    "    if dataset_key == \"trivia_qa\":\n",
    "        dataset = load_dataset(dataset_key, \"rc.web.nocontext\", split='train[:10000]')\n",
    "    else:\n",
    "        dataset = load_dataset(dataset_key, split='train[:10000]')\n",
    "    dataset.save_to_disk(os.path.join(\"datasets\", dataset_key))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Repo card metadata block was not found. Setting CardData to empty.\n",
      "Generating train split: 38080804 examples [04:35, 138111.42 examples/s]\n",
      "Saving the dataset (1/1 shards): 100%|██████████| 10000/10000 [00:00<00:00, 182797.37 examples/s]\n"
     ]
    }
   ],
   "source": [
    "# Downloading passages\n",
    "os.makedirs(\"passages\", exist_ok=True)\n",
    "dataset = load_dataset('vocab-transformers/wiki-en-passages-20210101', split='train[:10000]')\n",
    "dataset.save_to_disk(os.path.join(\"passages\", 'wiki-20210101-10000p'))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
